# -*- coding: utf-8 -*-
"""tutorai.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P56Rl9NJu-jhYKJ4zuvjziFW_DpBG48s
"""

!pip install git+https://github.com/openai/whisper.git
!pip install transformers accelerate sentencepiece
!pip install TTS
!pip install torchaudio

from google.colab import files
uploaded = files.upload()

from huggingface_hub import login
login("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")

import whisper

whisper_model = whisper.load_model("base")

def transcribe_audio(path="0703.mp3"):
    result = whisper_model.transcribe(path)
    return result["text"]

input_text = transcribe_audio("0703.mp3")
print("User said:", input_text)

from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

model_id = "mistralai/Mistral-7B-Instruct-v0.1"

tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)
model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto", torch_dtype="auto")

generator = pipeline("text-generation", model=model, tokenizer=tokenizer)

prompt = f"You are a friendly local language tutor.\nUser: {input_text}\nTutor:"
output = generator(prompt, max_new_tokens=150, do_sample=True)
response = output[0]["generated_text"]
print("Tutor says:", response)

from TTS.api import TTS

tts = TTS(model_name="tts_models/en/ljspeech/tacotron2-DDC", progress_bar=False)

# You can try a multilingual model too
# tts = TTS(model_name="tts_models/multilingual/multi-dataset/your_tts")

tts.tts_to_file(text=response, file_path="reply.wav")

import IPython.display as ipd
import torchaudio

audio_data, sample_rate = torchaudio.load("reply.wav")
ipd.Audio(audio_data.squeeze().numpy(), rate=sample_rate)

# Assuming the previous cell defining 'output' runs successfully
# The output structure is a list of dictionaries, each with a 'generated_text' key
generated_text = output[0]["generated_text"]

# The model's response starts after the "Tutor:" in the prompt
# We need to find the index of "Tutor:" and take the text after that
prompt_end_index = generated_text.find("Tutor:")
if prompt_end_index != -1:
    model_response = generated_text[prompt_end_index + len("Tutor:"):].strip()
else:
    # Fallback in case "Tutor:" is not found (shouldn't happen with the current prompt)
    model_response = generated_text.strip()

print(model_response)

"""# Task
Create a user interface for a web application using Streamlit that allows a user to upload an audio file, transcribe the audio using Whisper, generate a text response using a language model, and then generate and play an audio response using TTS. Provide the code and instructions for deployment on Streamlit Cloud.

## Install streamlit and other necessary libraries

### Subtask:
Install Streamlit and any other libraries your project needs (like `whisper`, `transformers`, `TTS`, `torch`, `torchaudio`) in your Python environment.

**Reasoning**:
Install the necessary libraries using pip.
"""

!pip install streamlit
!pip install whisper transformers TTS torch torchaudio

"""## Structure your code for a streamlit app

### Subtask:
Organize your code into a single Python script (`app.py` is a common name) that Streamlit can run. This will involve removing Colab-specific code (`google.colab.files`, `IPython.display`).

**Reasoning**:
Create the app.py file and add the necessary imports and function definitions for the core logic, excluding Colab-specific code.
"""

import streamlit as st
import whisper
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from TTS.api import TTS
import torchaudio
import numpy as np
import os

# Load models efficiently
@st.cache_resource
def load_whisper_model():
    return whisper.load_model("base")

@st.cache_resource
def load_llm_model():
    model_id = "mistralai/Mistral-7B-Instruct-v0.1"
    tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)
    model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto", torch_dtype="auto")
    generator = pipeline("text-generation", model=model, tokenizer=tokenizer)
    return tokenizer, model, generator

@st.cache_resource
def load_tts_model():
    return TTS(model_name="tts_models/en/ljspeech/tacotron2-DDC", progress_bar=False)

whisper_model = load_whisper_model()
tokenizer, model, generator = load_llm_model()
tts_model = load_tts_model()

def transcribe_audio(audio_path):
    result = whisper_model.transcribe(audio_path)
    return result["text"]

def generate_response(input_text, generator):
    prompt = f"You are a friendly local language tutor.\nUser: {input_text}\nTutor:"
    output = generator(prompt, max_new_tokens=150, do_sample=True, pad_token_id=generator.tokenizer.eos_token_id)
    response = output[0]["generated_text"]
    # Extract only the tutor's response
    prompt_end_index = response.find("Tutor:")
    if prompt_end_index != -1:
        model_response = response[prompt_end_index + len("Tutor:"):].strip()
    else:
        model_response = response.strip()
    return model_response

def synthesize_speech(text, tts_model, output_path="reply.wav"):
    tts_model.tts_to_file(text=text, file_path=output_path)
    return output_path

# Streamlit app layout will be added later

"""## Implement file upload in streamlit

### Subtask:
Replace the `google.colab.files.upload()` functionality with Streamlit's file uploader component (`st.file_uploader`).

**Reasoning**:
Implement the file uploader in the Streamlit app and add a conditional check for file upload.
"""

st.title("Language Tutor App")

uploaded_file = st.file_uploader("Upload an audio file", type=['mp3', 'wav'])

if uploaded_file is not None:
    # Process the uploaded file
    pass

"""**Reasoning**:
Save the uploaded audio file to a temporary location so it can be processed by Whisper.


"""

if uploaded_file is not None:
    # To read file as bytes:
    audio_bytes = uploaded_file.getvalue()

    # Save the bytes to a temporary file
    with open("temp_audio.wav", "wb") as f:
        f.write(audio_bytes)

    st.success("File uploaded successfully!")
    # Process the temporary file
    # The next steps will involve calling transcribe_audio with "temp_audio.wav"

"""## Integrate whisper for audio transcription

### Subtask:
Adapt the Whisper transcription code to work within the Streamlit app flow, taking the uploaded audio file as input.

**Reasoning**:
Call the transcribe_audio function with the path to the temporary audio file, store the result, and display it using Streamlit.
"""

if uploaded_file is not None:
    # To read file as bytes:
    audio_bytes = uploaded_file.getvalue()

    # Save the bytes to a temporary file
    temp_file_path = "temp_audio.wav"
    with open(temp_file_path, "wb") as f:
        f.write(audio_bytes)

    st.success("File uploaded successfully!")
    st.audio(audio_bytes, format="audio/wav") # Display the uploaded audio

    st.info("Transcribing audio...")
    # Transcribe the audio
    input_text = transcribe_audio(temp_file_path)
    st.text_area("Transcription:", input_text, height=150)

    # Clean up the temporary file (optional but good practice)
    # os.remove(temp_file_path)

"""## Integrate the language model for text generation

### Subtask:
Integrate the code for loading and using the Mistral model (or a smaller, more deployment-friendly model if needed) for text generation based on the transcribed text.

**Reasoning**:
Integrate the language model to generate a response based on the transcribed text and display it in the Streamlit app.
"""

if uploaded_file is not None:
    # To read file as bytes:
    audio_bytes = uploaded_file.getvalue()

    # Save the bytes to a temporary file
    temp_file_path = "temp_audio.wav"
    with open(temp_file_path, "wb") as f:
        f.write(audio_bytes)

    st.success("File uploaded successfully!")
    st.audio(audio_bytes, format="audio/wav") # Display the uploaded audio

    st.info("Transcribing audio...")
    # Transcribe the audio
    input_text = transcribe_audio(temp_file_path)
    st.text_area("Transcription:", input_text, height=150)

    st.info("Generating response...")
    # Generate response using the language model
    model_response = generate_response(input_text, generator)
    st.text_area("Tutor's Response:", model_response, height=150)

    # Clean up the temporary file (optional but good practice)
    # os.remove(temp_file_path)

"""## Integrate tts for audio response

### Subtask:
Adapt the TTS code to generate an audio file from the language model's response.

**Reasoning**:
Adapt the TTS code to generate an audio file from the language model's response by adding the speech synthesis section within the file upload block, including a status message, calling the `synthesize_speech` function, and displaying the audio player.
"""

import streamlit as st
import whisper
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from TTS.api import TTS
import torchaudio
import numpy as np
import os

# Load models efficiently
@st.cache_resource
def load_whisper_model():
    return whisper.load_model("base")

@st.cache_resource
def load_llm_model():
    model_id = "mistralai/Mistral-7B-Instruct-v0.1"
    tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)
    model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto", torch_dtype="auto")
    generator = pipeline("text-generation", model=model, tokenizer=tokenizer)
    return tokenizer, model, generator

@st.cache_resource
def load_tts_model():
    return TTS(model_name="tts_models/en/ljspeech/tacotron2-DDC", progress_bar=False)

whisper_model = load_whisper_model()
tokenizer, model, generator = load_llm_model()
tts_model = load_tts_model()

def transcribe_audio(audio_path):
    result = whisper_model.transcribe(audio_path)
    return result["text"]

def generate_response(input_text, generator):
    prompt = f"You are a friendly local language tutor.\nUser: {input_text}\nTutor:"
    output = generator(prompt, max_new_tokens=150, do_sample=True, pad_token_id=generator.tokenizer.eos_token_id)
    response = output[0]["generated_text"]
    # Extract only the tutor's response
    prompt_end_index = response.find("Tutor:")
    if prompt_end_index != -1:
        model_response = response[prompt_end_index + len("Tutor:"):].strip()
    else:
        model_response = response.strip()
    return model_response

def synthesize_speech(text, tts_model, output_path="reply.wav"):
    # Simple cleaning: remove non-alphanumeric characters except spaces
    # This is a basic approach; a more robust solution might involve language-specific cleaning or TTS models
    cleaned_text = "".join([c for c in text if c.isalnum() or c.isspace() or c in ".,!?'\""])
    tts_model.tts_to_file(text=cleaned_text, file_path=output_path)
    return output_path

st.title("Language Tutor App")

uploaded_file = st.file_uploader("Upload an audio file", type=["mp3", "wav"])

if uploaded_file is not None:
    # To read file as bytes:
    audio_bytes = uploaded_file.getvalue()

    # Save the bytes to a temporary file
    temp_file_path = "temp_audio.wav"
    with open(temp_file_path, "wb") as f:
        f.write(audio_bytes)

    st.success("File uploaded successfully!")
    st.audio(audio_bytes, format="audio/wav") # Display the uploaded audio

    st.info("Transcribing audio...")
    # Transcribe the audio
    input_text = transcribe_audio(temp_file_path)
    st.text_area("Transcription:", input_text, height=150)

    st.info("Generating response...")
    # Generate response using the language model
    model_response = generate_response(input_text, generator)
    st.text_area("Tutor's Response:", model_response, height=150)

    st.info("Synthesizing speech response...")
    # Synthesize speech from the model response
    audio_response_path = synthesize_speech(model_response, tts_model, "reply.wav")

    st.audio(audio_response_path)

    # Clean up the temporary files (optional but good practice)
    if os.path.exists(temp_file_path):
        os.remove(temp_file_path)
    if os.path.exists(audio_response_path):
         # Keep the reply.wav for playback, but could remove after
         pass # os.remove(audio_response_path)

"""## Display results and play audio in streamlit

### Subtask:
Use Streamlit components (`st.text_area`, `st.audio`) to display the transcribed text, the generated text response, and play the generated audio.

## Add user interface elements

### Subtask:
Add Streamlit widgets like buttons (`st.button`) to trigger the transcription, generation, and playback processes.

**Reasoning**:
Add a Streamlit button to trigger the audio processing and response generation, and include status messages to indicate the progress of each step.
"""

import streamlit as st
import whisper
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from TTS.api import TTS
import torchaudio
import numpy as np
import os

# Load models efficiently
@st.cache_resource
def load_whisper_model():
    return whisper.load_model("base")

@st.cache_resource
def load_llm_model():
    model_id = "mistralai/Mistral-7B-Instruct-v0.1"
    tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)
    model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto", torch_dtype="auto")
    generator = pipeline("text-generation", model=model, tokenizer=tokenizer)
    return tokenizer, model, generator

@st.cache_resource
def load_tts_model():
    return TTS(model_name="tts_models/en/ljspeech/tacotron2-DDC", progress_bar=False)

whisper_model = load_whisper_model()
tokenizer, model, generator = load_llm_model()
tts_model = load_tts_model()

def transcribe_audio(audio_path):
    result = whisper_model.transcribe(audio_path)
    return result["text"]

def generate_response(input_text, generator):
    prompt = f"You are a friendly local language tutor.\nUser: {input_text}\nTutor:"
    output = generator(prompt, max_new_tokens=150, do_sample=True, pad_token_id=generator.tokenizer.eos_token_id)
    response = output[0]["generated_text"]
    # Extract only the tutor's response
    prompt_end_index = response.find("Tutor:")
    if prompt_end_index != -1:
        model_response = response[prompt_end_index + len("Tutor:"):].strip()
    else:
        model_response = response.strip()
    return model_response

def synthesize_speech(text, tts_model, output_path="reply.wav"):
    # Simple cleaning: remove non-alphanumeric characters except spaces
    # This is a basic approach; a more robust solution might involve language-specific cleaning or TTS models
    cleaned_text = "".join([c for c in text if c.isalnum() or c.isspace() or c in ".,!?'\""])
    tts_model.tts_to_file(text=cleaned_text, file_path=output_path)
    return output_path

st.title("Language Tutor App")

uploaded_file = st.file_uploader("Upload an audio file", type=["mp3", "wav"])

if uploaded_file is not None:
    st.success("File uploaded successfully!")
    audio_bytes = uploaded_file.getvalue()
    st.audio(audio_bytes, format="audio/wav")

    if st.button("Process Audio"):
        # Save the bytes to a temporary file
        temp_file_path = "temp_audio.wav"
        with open(temp_file_path, "wb") as f:
            f.write(audio_bytes)

        with st.spinner("Transcribing audio..."):
            # Transcribe the audio
            input_text = transcribe_audio(temp_file_path)
        st.text_area("Transcription:", input_text, height=150)

        with st.spinner("Generating response..."):
            # Generate response using the language model
            model_response = generate_response(input_text, generator)
        st.text_area("Tutor's Response:", model_response, height=150)

        with st.spinner("Synthesizing speech response..."):
            # Synthesize speech from the model response
            audio_response_path = synthesize_speech(model_response, tts_model, "reply.wav")
        st.audio(audio_response_path)

        # Clean up the temporary file (optional but good practice)
        if os.path.exists(temp_file_path):
            os.remove(temp_file_path)
        if os.path.exists(audio_response_path):
             # Keep the reply.wav for playback, but could remove after
             pass # os.remove(audio_response_path)

"""## Refine and test the streamlit app

### Subtask:
Refine and test the streamlit app

**Reasoning**:
Review and refine the app.py script based on the instructions, including adding error handling for file uploads, considering a smaller LLM if resource issues arise during testing, and ensuring temporary files are handled correctly. The current code saves the uploaded file as 'temp_audio.wav' which assumes the input is always wav; it needs to handle different input file types and potentially convert them to a format suitable for Whisper (16kHz mono WAV). I will also add error handling around the file upload and processing steps.
"""

import streamlit as st
import whisper
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from TTS.api import TTS
import torchaudio
import numpy as np
import os
import tempfile

# Load models efficiently
# Use a smaller Whisper model if needed for deployment
@st.cache_resource
def load_whisper_model():
    try:
        return whisper.load_model("base") # Consider "small" or "tiny" for smaller models
    except Exception as e:
        st.error(f"Error loading Whisper model: {e}")
        return None

# Consider a smaller LLM if needed for deployment due to resource constraints
@st.cache_resource
def load_llm_model():
    # model_id = "mistralai/Mistral-7-Instruct-v0.1" # Large model
    model_id = "gpt2" # Smaller model for testing/deployment on limited resources
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)
        model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto", torch_dtype="auto")
        # Ensure pad_token_id is set for open-end generation with GPT2
        if tokenizer.pad_token_id is None:
            tokenizer.pad_token_id = tokenizer.eos_token_id
        generator = pipeline("text-generation", model=model, tokenizer=tokenizer)
        return tokenizer, model, generator
    except Exception as e:
        st.error(f"Error loading Language Model: {e}")
        return None, None, None


# Use a smaller TTS model if needed for deployment
@st.cache_resource
def load_tts_model():
    try:
        # Consider other models if en/ljspeech/tacotron2-DDC is too large or slow
        return TTS(model_name="tts_models/en/ljspeech/tacotron2-DDC", progress_bar=False)
    except Exception as e:
        st.error(f"Error loading TTS model: {e}")
        return None

# Function to convert audio to 16kHz mono WAV
def convert_audio_for_whisper(audio_path):
    try:
        y, sr = torchaudio.load(audio_path)
        # Resample if necessary
        if sr != 16000:
            resampler = torchaudio.transforms.Resample(sr, 16000)
            y = resampler(y)
        # Convert to mono if necessary
        if y.shape[0] > 1:
            y = torch.mean(y, dim=0, keepdim=True)
        # Save as WAV
        converted_path = audio_path + "_converted.wav"
        torchaudio.save(converted_path, y, 16000)
        return converted_path
    except Exception as e:
        st.error(f"Error converting audio file: {e}")
        return None

whisper_model = load_whisper_model()
tokenizer, model, generator = load_llm_model()
tts_model = load_tts_model()

def transcribe_audio(audio_path):
    if whisper_model is None:
        return "Whisper model not loaded."
    try:
        result = whisper_model.transcribe(audio_path)
        return result["text"]
    except Exception as e:
        st.error(f"Error during transcription: {e}")
        return "Transcription failed."

def generate_response(input_text, generator):
    if generator is None:
        return "Language model not loaded."
    try:
        # Check if input_text is empty or too short
        if not input_text or len(input_text.strip()) < 5: # Simple check for meaningful input
             return "Please provide more input for a meaningful response."

        prompt = f"You are a friendly local language tutor.\nUser: {input_text}\nTutor:"
        # Adjust max_new_tokens and other generation parameters if needed
        output = generator(prompt, max_new_tokens=150, do_sample=True, pad_token_id=generator.tokenizer.eos_token_id)
        response = output[0]["generated_text"]
        # Extract only the tutor's response
        prompt_end_index = response.find("Tutor:")
        if prompt_end_index != -1:
            model_response = response[prompt_end_index + len("Tutor:"):].strip()
        else:
            # Fallback if the prompt structure isn't perfectly maintained by the LLM
            model_response = response.strip()
        return model_response
    except Exception as e:
        st.error(f"Error during response generation: {e}")
        return "Response generation failed."


def synthesize_speech(text, tts_model, output_path):
    if tts_model is None:
        return "TTS model not loaded."
    try:
        # Simple cleaning: remove characters that might cause issues with TTS
        # This might need refinement based on the specific TTS model and expected languages
        cleaned_text = "".join([c for c in text if c.isalnum() or c.isspace() or c in ".,!?'\""])
        if not cleaned_text.strip():
             return None # Don't synthesize if text is empty after cleaning

        tts_model.tts_to_file(text=cleaned_text, file_path=output_path)
        return output_path
    except Exception as e:
        st.error(f"Error during speech synthesis: {e}")
        return None

st.title("Language Tutor App")
st.write("Upload an audio file and get a language tutor's response!")

uploaded_file = st.file_uploader("Upload an audio file", type=["mp3", "wav"])

if uploaded_file is not None:
    st.success("File uploaded successfully!")
    audio_bytes = uploaded_file.getvalue()
    # Display the uploaded audio - format might need adjustment based on actual file type
    st.audio(audio_bytes, format=uploaded_file.type)

    if st.button("Process Audio"):
        # Use tempfile to handle temporary files more robustly
        with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(uploaded_file.name)[1]) as tmp_file:
            tmp_file.write(audio_bytes)
            temp_file_path = tmp_file.name

        converted_audio_path = None
        try:
            with st.spinner("Converting audio..."):
                 # Convert audio to suitable format for Whisper
                 converted_audio_path = convert_audio_for_whisper(temp_file_path)

            if converted_audio_path:
                with st.spinner("Transcribing audio..."):
                    # Transcribe the audio
                    input_text = transcribe_audio(converted_audio_path)
                st.text_area("Transcription:", input_text, height=150)

                with st.spinner("Generating response..."):
                    # Generate response using the language model
                    model_response = generate_response(input_text, generator)
                st.text_area("Tutor's Response:", model_response, height=150)

                if model_response and model_response != "Response generation failed." and model_response != "Please provide more input for a meaningful response.":
                    with st.spinner("Synthesizing speech response..."):
                        # Use another tempfile for the synthesized audio
                        with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as audio_res_tmp:
                            audio_response_path = synthesize_speech(model_response, tts_model, audio_res_tmp.name)

                    if audio_response_path and os.path.exists(audio_response_path):
                        st.audio(audio_response_path)
                        # Clean up the synthesized audio temp file after playback
                        os.remove(audio_response_path)
                    elif audio_response_path is None:
                         st.info("No speech synthesized (response might be empty after cleaning).")
                elif model_response == "Please provide more input for a meaningful response.":
                     st.info(model_response)


        finally:
            # Clean up temporary files
            if os.path.exists(temp_file_path):
                os.remove(temp_file_path)
            if converted_audio_path and os.path.exists(converted_audio_path):
                os.remove(converted_audio_path)

elif uploaded_file is None:
    st.info("Please upload an audio file to begin.")

# Add instructions for deployment
st.markdown("""
### Deployment Instructions

To deploy this app on Streamlit Cloud:

1.  **Save the code:** Save the code above as `app.py`.
2.  **Create a `requirements.txt` file:** Create a file named `requirements.txt` in the same directory as `app.py` with the following content:
    ```
    streamlit
    openai-whisper
    transformers
    accelerate
    sentencepiece
    TTS
    torch
    torchaudio
    numpy
    ```
3.  **Create a GitHub repository:** Push both `app.py` and `requirements.txt` to a new GitHub repository.
4.  **Deploy on Streamlit Cloud:** Go to the [Streamlit Cloud website](https://streamlit.io/cloud) and click "Deploy an app". Connect your GitHub account, select the repository, choose the branch (usually `main` or `master`), and set the main file path to `app.py`. Click "Deploy!".

**Note:** Depending on the size of the models ("base" Whisper, "Mistral-7B", "tacotron2-DDC"), deployment on free tiers of Streamlit Cloud might exceed resource limits (CPU, RAM). You might need to use smaller models (e.g., "tiny" or "small" Whisper, "gpt2" or similar smaller LLMs, or a more lightweight TTS model) or consider paid hosting options. The current code uses "gpt2" and "base" Whisper which *might* fit within limits but testing is required.
""")

"""## Deploy the streamlit app

### Subtask:
Deploy your Streamlit application to a hosting platform (e.g., Streamlit Cloud, Hugging Face Spaces, Heroku, etc.).

## Summary:

### Data Analysis Key Findings

*   All necessary Python libraries (`streamlit`, `whisper`, `transformers`, `TTS`, `torch`, `torchaudio`) were successfully installed.
*   The code for the Streamlit application was organized into a single Python script (`app.py`), removing Colab-specific dependencies and utilizing `@st.cache_resource` for efficient model loading.
*   Streamlit's `st.file_uploader` was successfully implemented to handle audio file uploads, saving the uploaded audio to a temporary file.
*   The Whisper transcription code was integrated to transcribe the uploaded audio, and the resulting text was displayed using `st.text_area`.
*   The language model was integrated to generate a text response based on the transcription, and this response was also displayed using `st.text_area`.
*   The TTS code was integrated to synthesize speech from the language model's response, saving the audio to a temporary file and displaying it using `st.audio`.
*   Basic character cleaning was added to the text before TTS synthesis to improve compatibility.
*   A "Process Audio" button (`st.button`) was added to trigger the entire audio processing pipeline.
*   `st.spinner` was used to provide visual feedback to the user during transcription, text generation, and speech synthesis.
*   The code includes error handling for model loading and processing steps, and uses `tempfile` for more robust temporary file management.
*   Audio conversion to 16kHz mono WAV using `torchaudio` was implemented to ensure compatibility with Whisper.
*   Instructions for deploying the application on Streamlit Cloud, including creating `requirements.txt` and using GitHub, were provided within the application's markdown.

### Insights or Next Steps

*   The current application uses relatively large models ("base" Whisper, "gpt2", "tacotron2-DDC"). To ensure successful deployment on free tiers of hosting platforms like Streamlit Cloud, consider replacing these with even smaller, more resource-efficient models (e.g., "tiny" Whisper, a smaller finetuned language model, or a different TTS model).
*   Implement more sophisticated text cleaning before TTS synthesis, potentially using natural language processing techniques, to handle a wider variety of characters and phrases that might cause issues with the TTS model.

## Install streamlit and other necessary libraries

### Subtask:
Install Streamlit and any other libraries your project needs (like `whisper`, `transformers`, `TTS`, `torch`, `torchaudio`) in your Python environment.

**Reasoning**:
Install the necessary libraries using pip.
"""

!pip install streamlit
!pip install whisper transformers TTS torch torchaudio

"""## Structure your code for a streamlit app

### Subtask:
Organize your code into a single Python script (`app.py` is a common name) that Streamlit can run. This will involve removing Colab-specific code (`google.colab.files`, `IPython.display`).

**Reasoning**:
Create the app.py file and add the necessary imports and function definitions for the core logic, excluding Colab-specific code.
"""

import streamlit as st
import whisper
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from TTS.api import TTS
import torchaudio
import numpy as np
import os

# Load models efficiently
@st.cache_resource
def load_whisper_model():
    return whisper.load_model("base")

@st.cache_resource
def load_llm_model():
    # model_id = "mistralai/Mistral-7B-Instruct-v0.1" # Original large model
    model_id = "gpt2" # Smaller model for better compatibility with limited resources
    tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)
    model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto", torch_dtype="auto")
    # Ensure pad_token_id is set for open-end generation with GPT2
    if tokenizer.pad_token_id is None:
        tokenizer.pad_token_id = tokenizer.eos_token_id
    generator = pipeline("text-generation", model=model, tokenizer=tokenizer)
    return tokenizer, model, generator

@st.cache_resource
def load_tts_model():
    return TTS(model_name="tts_models/en/ljspeech/tacotron2-DDC", progress_bar=False)


# Load models when the app starts
whisper_model = load_whisper_model()
tokenizer, model, generator = load_llm_model()
tts_model = load_tts_model()


def transcribe_audio(audio_path):
    # Placeholder for Whisper transcription logic
    result = whisper_model.transcribe(audio_path)
    return result["text"]

def generate_response(input_text, generator):
    # Placeholder for language model text generation logic
    prompt = f"You are a friendly local language tutor.\nUser: {input_text}\nTutor:"
    # Note: The generation parameters might need tuning
    output = generator(prompt, max_new_tokens=150, do_sample=True, pad_token_id=generator.tokenizer.eos_token_id) # Added pad_token_id for compatibility
    response = output[0]["generated_text"]
    # Extract only the tutor's response (logic from previous notebook cell)
    prompt_end_index = response.find("Tutor:")
    if prompt_end_index != -1:
        model_response = response[prompt_end_index + len("Tutor:"):].strip()
    else:
        model_response = response.strip()
    return model_response


def synthesize_speech(text, tts_model, output_path="reply.wav"):
    # Placeholder for TTS speech synthesis logic
    # This might need additional error handling or text cleaning for the TTS model
    tts_model.tts_to_file(text=text, file_path=output_path)
    return output_path

# Add Streamlit UI elements in the next steps

"""## Implement file upload in streamlit

### Subtask:
Replace the `google.colab.files.upload()` functionality with Streamlit's file uploader component (`st.file_uploader`).

**Reasoning**:
Implement the file uploader in the Streamlit app and add a conditional check for file upload.
"""

st.title("Language Tutor App")

uploaded_file = st.file_uploader("Upload an audio file", type=['mp3', 'wav'])

if uploaded_file is not None:
    # Process the uploaded file - we'll add this logic in the next steps
    pass

"""**Reasoning**:
Save the uploaded audio file to a temporary location so it can be processed by Whisper.
"""

if uploaded_file is not None:
    # To read file as bytes:
    audio_bytes = uploaded_file.getvalue()

    # Save the bytes to a temporary file
    with open("temp_audio.wav", "wb") as f:
        f.write(audio_bytes)

    st.success("File uploaded successfully!")
    # Process the temporary file
    # The next steps will involve calling transcribe_audio with "temp_audio.wav"

"""## Integrate whisper for audio transcription

### Subtask:
Adapt the Whisper transcription code to work within the Streamlit app flow, taking the uploaded audio file as input.

**Reasoning**:
Call the transcribe_audio function with the path to the temporary audio file, store the result, and display it using Streamlit.
"""

if uploaded_file is not None:
    # To read file as bytes:
    audio_bytes = uploaded_file.getvalue()

    # Save the bytes to a temporary file
    temp_file_path = "temp_audio.wav"
    with open(temp_file_path, "wb") as f:
        f.write(audio_bytes)

    st.success("File uploaded successfully!")
    st.audio(audio_bytes, format="audio/wav") # Display the uploaded audio

    st.info("Transcribing audio...")
    # Transcribe the audio
    input_text = transcribe_audio(temp_file_path)
    st.text_area("Transcription:", input_text, height=150)

    # Clean up the temporary file (optional but good practice)
    # os.remove(temp_file_path)

"""## Integrate the language model for text generation

### Subtask:
Integrate the code for loading and using the Mistral model (or a smaller, more deployment-friendly model if needed) for text generation based on the transcribed text.

**Reasoning**:
Integrate the language model to generate a response based on the transcribed text and display it in the Streamlit app.
"""

if uploaded_file is not None:
    # To read file as bytes:
    audio_bytes = uploaded_file.getvalue()

    # Save the bytes to a temporary file
    temp_file_path = "temp_audio.wav"
    with open(temp_file_path, "wb") as f:
        f.write(audio_bytes)

    st.success("File uploaded successfully!")
    st.audio(audio_bytes, format="audio/wav") # Display the uploaded audio

    st.info("Transcribing audio...")
    # Transcribe the audio
    input_text = transcribe_audio(temp_file_path)
    st.text_area("Transcription:", input_text, height=150)

    st.info("Generating response...")
    # Generate response using the language model
    model_response = generate_response(input_text, generator)
    st.text_area("Tutor's Response:", model_response, height=150)

    # Clean up the temporary file (optional but good practice)
    # os.remove(temp_file_path)

"""## Integrate tts for audio response

### Subtask:
Adapt the TTS code to generate an audio file from the language model's response.

**Reasoning**:
Adapt the TTS code to generate an audio file from the language model's response by adding the speech synthesis section within the file upload block, including a status message, calling the `synthesize_speech` function, and displaying the audio player.
"""

import streamlit as st
import whisper
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from TTS.api import TTS
import torchaudio
import numpy as np
import os

# Load models efficiently
@st.cache_resource
def load_whisper_model():
    return whisper.load_model("base")

@st.cache_resource
def load_llm_model():
    # model_id = "mistralai/Mistral-7B-Instruct-v0.1" # Original large model
    model_id = "gpt2" # Smaller model for better compatibility with limited resources
    tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)
    model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto", torch_dtype="auto")
    # Ensure pad_token_id is set for open-end generation with GPT2
    if tokenizer.pad_token_id is None:
        tokenizer.pad_token_id = tokenizer.eos_token_id
    generator = pipeline("text-generation", model=model, tokenizer=tokenizer)
    return tokenizer, model, generator

@st.cache_resource
def load_tts_model():
    return TTS(model_name="tts_models/en/ljspeech/tacotron2-DDC", progress_bar=False)

whisper_model = load_whisper_model()
tokenizer, model, generator = load_llm_model()
tts_model = load_tts_model()

def transcribe_audio(audio_path):
    result = whisper_model.transcribe(audio_path)
    return result["text"]

def generate_response(input_text, generator):
    prompt = f"You are a friendly local language tutor.\nUser: {input_text}\nTutor:"
    output = generator(prompt, max_new_tokens=150, do_sample=True, pad_token_id=generator.tokenizer.eos_token_id)
    response = output[0]["generated_text"]
    # Extract only the tutor's response
    prompt_end_index = response.find("Tutor:")
    if prompt_end_index != -1:
        model_response = response[prompt_end_index + len("Tutor:"):].strip()
    else:
        model_response = response.strip()
    return model_response

def synthesize_speech(text, tts_model, output_path="reply.wav"):
    # Simple cleaning: remove non-alphanumeric characters except spaces
    # This is a basic approach; a more robust solution might involve language-specific cleaning or TTS models
    cleaned_text = "".join([c for c in text if c.isalnum() or c.isspace() or c in ".,!?'\""])
    tts_model.tts_to_file(text=cleaned_text, file_path=output_path)
    return output_path

st.title("Language Tutor App")

uploaded_file = st.file_uploader("Upload an audio file", type=["mp3", "wav"])

if uploaded_file is not None:
    # To read file as bytes:
    audio_bytes = uploaded_file.getvalue()

    # Save the bytes to a temporary file
    temp_file_path = "temp_audio.wav"
    with open(temp_file_path, "wb") as f:
        f.write(audio_bytes)

    st.success("File uploaded successfully!")
    st.audio(audio_bytes, format="audio/wav") # Display the uploaded audio

    st.info("Transcribing audio...")
    # Transcribe the audio
    input_text = transcribe_audio(temp_file_path)
    st.text_area("Transcription:", input_text, height=150)

    st.info("Generating response...")
    # Generate response using the language model
    model_response = generate_response(input_text, generator)
    st.text_area("Tutor's Response:", model_response, height=150)

    st.info("Synthesizing speech response...")
    # Synthesize speech from the model response
    audio_response_path = synthesize_speech(model_response, tts_model, "reply.wav")

    st.audio(audio_response_path)

    # Clean up the temporary files (optional but good practice)
    if os.path.exists(temp_file_path):
        os.remove(temp_file_path)
    if os.path.exists(audio_response_path):
         # Keep the reply.wav for playback, but could remove after
         pass # os.remove(audio_response_path)

"""## Display results and play audio in streamlit

### Subtask:
Use Streamlit components (`st.text_area`, `st.audio`) to display the transcribed text, the generated text response, and play the generated audio.

## Add user interface elements

### Subtask:
Add Streamlit widgets like buttons (`st.button`) to trigger the transcription, generation, and playback processes.

**Reasoning**:
Add a Streamlit button to trigger the audio processing and response generation, and include status messages to indicate the progress of each step.
"""

import streamlit as st
import whisper
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from TTS.api import TTS
import torchaudio
import numpy as np
import os

# Load models efficiently
@st.cache_resource
def load_whisper_model():
    return whisper.load_model("base")

@st.cache_resource
def load_llm_model():
    # model_id = "mistralai/Mistral-7B-Instruct-v0.1" # Original large model
    model_id = "gpt2" # Smaller model for better compatibility with limited resources
    tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)
    model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto", torch_dtype="auto")
    # Ensure pad_token_id is set for open-end generation with GPT2
    if tokenizer.pad_token_id is None:
        tokenizer.pad_token_id = tokenizer.eos_token_id
    generator = pipeline("text-generation", model=model, tokenizer=tokenizer)
    return tokenizer, model, generator

@st.cache_resource
def load_tts_model():
    return TTS(model_name="tts_models/en/ljspeech/tacotron2-DDC", progress_bar=False)

whisper_model = load_whisper_model()
tokenizer, model, generator = load_llm_model()
tts_model = load_tts_model()

def transcribe_audio(audio_path):
    result = whisper_model.transcribe(audio_path)
    return result["text"]

def generate_response(input_text, generator):
    prompt = f"You are a friendly local language tutor.\nUser: {input_text}\nTutor:"
    output = generator(prompt, max_new_tokens=150, do_sample=True, pad_token_id=generator.tokenizer.eos_token_id)
    response = output[0]["generated_text"]
    # Extract only the tutor's response
    prompt_end_index = response.find("Tutor:")
    if prompt_end_index != -1:
        model_response = response[prompt_end_index + len("Tutor:"):].strip()
    else:
        model_response = response.strip()
    return model_response

def synthesize_speech(text, tts_model, output_path="reply.wav"):
    # Simple cleaning: remove non-alphanumeric characters except spaces
    # This is a basic approach; a more robust solution might involve language-specific cleaning or TTS models
    cleaned_text = "".join([c for c in text if c.isalnum() or c.isspace() or c in ".,!?'\""])
    tts_model.tts_to_file(text=cleaned_text, file_path=output_path)
    return output_path

st.title("Language Tutor App")

uploaded_file = st.file_uploader("Upload an audio file", type=["mp3", "wav"])

if uploaded_file is not None:
    st.success("File uploaded successfully!")
    audio_bytes = uploaded_file.getvalue()
    st.audio(audio_bytes, format="audio/wav")

    if st.button("Process Audio"):
        # Save the bytes to a temporary file
        temp_file_path = "temp_audio.wav"
        with open(temp_file_path, "wb") as f:
            f.write(audio_bytes)

        with st.spinner("Transcribing audio..."):
            # Transcribe the audio
            input_text = transcribe_audio(temp_file_path)
        st.text_area("Transcription:", input_text, height=150)

        with st.spinner("Generating response..."):
            # Generate response using the language model
            model_response = generate_response(input_text, generator)
        st.text_area("Tutor's Response:", model_response, height=150)

        with st.spinner("Synthesizing speech response..."):
            # Synthesize speech from the model response
            audio_response_path = synthesize_speech(model_response, tts_model, "reply.wav")
        st.audio(audio_response_path)

        # Clean up the temporary file (optional but good practice)
        if os.path.exists(temp_file_path):
            os.remove(temp_file_path)
        if os.path.exists(audio_response_path):
             # Keep the reply.wav for playback, but could remove after
             pass # os.remove(audio_response_path)

"""## Refine and test the streamlit app

### Subtask:
Refine and test the streamlit app

**Reasoning**:
Review and refine the app.py script based on the instructions, including adding error handling for file uploads, considering a smaller LLM if resource issues arise during testing, and ensuring temporary files are handled correctly. The current code saves the uploaded file as 'temp_audio.wav' which assumes the input is always wav; it needs to handle different input file types and potentially convert them to a format suitable for Whisper (16kHz mono WAV). I will also add error handling around the file upload and processing steps.
"""

import streamlit as st
import whisper
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from TTS.api import TTS
import torchaudio
import numpy as np
import os
import tempfile
import torch # Import torch for audio conversion

# Load models efficiently
# Use a smaller Whisper model if needed for deployment
@st.cache_resource
def load_whisper_model():
    try:
        return whisper.load_model("base") # Consider "small" or "tiny" for smaller models
    except Exception as e:
        st.error(f"Error loading Whisper model: {e}")
        return None

# Consider a smaller LLM if needed for deployment due to resource constraints
@st.cache_resource
def load_llm_model():
    # model_id = "mistralai/Mistral-7-Instruct-v0.1" # Large model
    model_id = "gpt2" # Smaller model for testing/deployment on limited resources
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)
        model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto", torch_dtype="auto")
        # Ensure pad_token_id is set for open-end generation with GPT2
        if tokenizer.pad_token_id is None:
            tokenizer.pad_token_id = tokenizer.eos_token_id
        generator = pipeline("text-generation", model=model, tokenizer=tokenizer)
        return tokenizer, model, generator
    except Exception as e:
        st.error(f"Error loading Language Model: {e}")
        return None, None, None


# Use a smaller TTS model if needed for deployment
@st.cache_resource
def load_tts_model():
    try:
        # Consider other models if en/ljspeech/tacotron2-DDC is too large or slow
        return TTS(model_name="tts_models/en/ljspeech/tacotron2-DDC", progress_bar=False)
    except Exception as e:
        st.error(f"Error loading TTS model: {e}")
        return None

# Function to convert audio to 16kHz mono WAV
def convert_audio_for_whisper(audio_path):
    try:
        y, sr = torchaudio.load(audio_path)
        # Resample if necessary
        if sr != 16000:
            resampler = torchaudio.transforms.Resample(sr, 16000)
            y = resampler(y)
        # Convert to mono if necessary
        if y.shape[0] > 1:
            y = torch.mean(y, dim=0, keepdim=True)
        # Save as WAV
        converted_path = audio_path + "_converted.wav"
        torchaudio.save(converted_path, y, 16000)
        return converted_path
    except Exception as e:
        st.error(f"Error converting audio file: {e}")
        return "Conversion failed." # Return a string message on failure

    return converted_path # Return the path on success


whisper_model = load_whisper_model()
tokenizer, model, generator = load_llm_model()
tts_model = load_tts_model()

def transcribe_audio(audio_path):
    if whisper_model is None:
        return "Whisper model not loaded."
    try:
        result = whisper_model.transcribe(audio_path)
        return result["text"]
    except Exception as e:
        st.error(f"Error during transcription: {e}")
        return "Transcription failed."

def generate_response(input_text, generator):
    if generator is None:
        return "Language model not loaded."
    try:
        # Check if input_text is empty or too short
        if not input_text or len(input_text.strip()) < 5: # Simple check for meaningful input
             return "Please provide more input for a meaningful response."

        prompt = f"You are a friendly local language tutor.\nUser: {input_text}\nTutor:"
        # Adjust max_new_tokens and other generation parameters if needed
        output = generator(prompt, max_new_tokens=150, do_sample=True, pad_token_id=generator.tokenizer.eos_token_id)
        response = output[0]["generated_text"]
        # Extract only the tutor's response
        prompt_end_index = response.find("Tutor:")
        if prompt_end_index != -1:
            model_response = response[prompt_end_index + len("Tutor:"):].strip()
        else:
            # Fallback if the prompt structure isn't perfectly maintained by the LLM
            model_response = response.strip()
        return model_response
    except Exception as e:
        st.error(f"Error during response generation: {e}")
        return "Response generation failed."


def synthesize_speech(text, tts_model, output_path):
    if tts_model is None:
        return "TTS model not loaded."
    try:
        # Simple cleaning: remove characters that might cause issues with TTS
        # This might need refinement based on the specific TTS model and expected languages
        cleaned_text = "".join([c for c in text if c.isalnum() or c.isspace() or c in ".,!?'\""])
        if not cleaned_text.strip():
             return None # Don't synthesize if text is empty after cleaning

        tts_model.tts_to_file(text=cleaned_text, file_path=output_path)
        return output_path
    except Exception as e:
        st.error(f"Error during speech synthesis: {e}")
        return None

st.title("Language Tutor App")
st.write("Upload an audio file and get a language tutor's response!")

uploaded_file = st.file_uploader("Upload an audio file", type=["mp3", "wav"])

if uploaded_file is not None:
    st.success("File uploaded successfully!")
    audio_bytes = uploaded_file.getvalue()
    # Display the uploaded audio - format might need adjustment based on actual file type
    st.audio(audio_bytes, format=uploaded_file.type)

    if st.button("Process Audio"):
        # Use tempfile to handle temporary files more robustly
        with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(uploaded_file.name)[1]) as tmp_file:
            tmp_file.write(audio_bytes)
            temp_file_path = tmp_file.name

        converted_audio_path = None
        try:
            with st.spinner("Converting audio..."):
                 # Convert audio to suitable format for Whisper
                 converted_audio_path = convert_audio_for_whisper(temp_file_path)

            if converted_audio_path and converted_audio_path != "Conversion failed.":
                with st.spinner("Transcribing audio..."):
                    # Transcribe the audio
                    input_text = transcribe_audio(converted_audio_path)
                st.text_area("Transcription:", input_text, height=150)

                with st.spinner("Generating response..."):
                    # Generate response using the language model
                    model_response = generate_response(input_text, generator)
                st.text_area("Tutor's Response:", model_response, height=150)

                if model_response and model_response != "Response generation failed." and model_response != "Please provide more input for a meaningful response.":
                    with st.spinner("Synthesizing speech response..."):
                        # Use another tempfile for the synthesized audio
                        with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as audio_res_tmp:
                            audio_response_path = synthesize_speech(model_response, tts_model, audio_res_tmp.name)

                    if audio_response_path and os.path.exists(audio_response_path):
                        st.audio(audio_response_path)
                        # Clean up the synthesized audio temp file after playback
                        os.remove(audio_response_path)
                    elif audio_response_path is None:
                         st.info("No speech synthesized (response might be empty after cleaning).")
                elif model_response == "Please provide more input for a meaningful response.":
                     st.info(model_response)


        finally:
            # Clean up temporary files
            if os.path.exists(temp_file_path):
                os.remove(temp_file_path)
            if converted_audio_path and os.path.exists(converted_audio_path):
                os.remove(converted_audio_path)

elif uploaded_file is None:
    st.info("Please upload an audio file to begin.")

# Add instructions for deployment
st.markdown("""
### Deployment Instructions

To deploy this app on Streamlit Cloud:

1.  **Save the code:** Save the code above as `app.py`.
2.  **Create a `requirements.txt` file:** Create a file named `requirements.txt` in the same directory as `app.py` with the following content:

"""## Deploy the streamlit app

### Subtask:
Deploy your Streamlit application to a hosting platform (e.g., Streamlit Cloud, Hugging Face Spaces, Heroku, etc.).

## Summary:

### Data Analysis Key Findings

* All necessary Python libraries (`streamlit`, `whisper`, `transformers`, `TTS`, `torch`, `torchaudio`) were successfully installed.
* The code for the Streamlit application was organized into a single Python script (`app.py`), removing Colab-specific dependencies and utilizing `@st.cache_resource` for efficient model loading.
* Streamlit's `st.file_uploader` was successfully implemented to handle audio file uploads, saving the uploaded audio to a temporary file.
* The Whisper transcription code was integrated to transcribe the uploaded audio, and the resulting text was displayed using `st.text_area`.
* The language model was integrated to generate a text response based on the transcription, and this response was also displayed using `st.text_area`.
* The TTS code was integrated to synthesize speech from the language model's response, saving the audio to a temporary file and displaying it using `st.audio`.
* Basic character cleaning was added to the text before TTS synthesis to improve compatibility.
* A "Process Audio" button (`st.button`) was added to trigger the entire audio processing pipeline.
* `st.spinner` was used to provide visual feedback to the user during transcription, text generation, and speech synthesis.
* The code includes error handling for model loading and processing steps, and uses `tempfile` for more robust temporary file management.
* Audio conversion to 16kHz mono WAV using `torchaudio` was implemented to ensure compatibility with Whisper.
* Instructions for deploying the application on Streamlit Cloud, including creating `requirements.txt` and using GitHub, were provided within the application's markdown.

### Insights or Next Steps

* The current application uses relatively large models ("base" Whisper, "gpt2", "tacotron2-DDC"). To ensure successful deployment on free tiers of hosting platforms like Streamlit Cloud, consider replacing these with even smaller, more resource-efficient models (e.g., "tiny" Whisper, a smaller finetuned language model, or a more lightweight TTS model).
* Implement more sophisticated text cleaning before TTS synthesis, potentially using natural language processing techniques, to handle a wider variety of characters and phrases that might cause issues with the TTS model.
"""